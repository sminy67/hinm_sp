modifiers:
  - !EpochRangeModifier
      start_epoch: 1.0
      end_epoch: 30.0

training_modifiers:
  - !LearningRateFunctionModifier
      cycle_epochs: 1.0
      end_epoch: 30.0
      final_lr: 1.5e-06
      init_lr: 0.00015
      lr_func: linear
      start_epoch: 1.0
      update_frequency: -1.0

  - !OBSnm16v64PruningModifier
    params: [
      "re:bert.encoder.layer.*.attention.self.query.weight",
      "re:bert.encoder.layer.*.attention.self.key.weight",
      "re:bert.encoder.layer.*.attention.self.value.weight",
      "re:bert.encoder.layer.*.attention.output.dense.weight",
      "re:bert.encoder.layer.*.intermediate.dense.weight",
      "re:bert.encoder.layer.*.output.dense.weight",
    ]
    init_sparsity: 0.875
    final_sparsity: 0.875
    start_epoch: 0
    end_epoch: 0
    update_frequency: 1.0
    inter_func: cubic
    global_sparsity: False
    mask_type: unstructured
    num_grads: 1024
    damp: 1e-7
    fisher_block_size: 16
    grad_sampler_kwargs:
      batch_size: 8

  - !ConstantPruningModifier
      start_epoch: 1.0
      end_epoch: 30.0
      params: ['re:bert.encoder.layer.*.attention.self.query.weight', 're:bert.encoder.layer.*.attention.self.key.weight', 're:bert.encoder.layer.*.attention.self.value.weight', 're:bert.encoder.layer.*.attention.output.dense.weight', 're:bert.encoder.layer.*.intermediate.dense.weight', 're:bert.encoder.layer.*.output.dense.weight']
      update_frequency: -1

distillation_modifiers:
  - !DistillationModifier
      distill_output_keys: ['start_logits', 'end_logits']
      end_epoch: -1.0
      hardness: 1.0
      start_epoch: 1.0
      temperature: 5.5
      update_frequency: -1.0
#pruning_modifiers:
#  - !OBSnmvPruningModifier
#    params: [
#      "re:bert.encoder.layer.*.attention.self.query.weight",
#      "re:bert.encoder.layer.*.attention.self.key.weight",
#      "re:bert.encoder.layer.*.attention.self.value.weight",
#      "re:bert.encoder.layer.*.attention.output.dense.weight",
#      "re:bert.encoder.layer.*.intermediate.dense.weight",
#      "re:bert.encoder.layer.*.output.dense.weight",
#    ]
#    init_sparsity: 0.875
#    final_sparsity: 0.875
#    start_epoch: 0
#    end_epoch: 1
#    update_frequency: 1.0
#    inter_func: cubic
#    global_sparsity: False
#    mask_type: unstructured
#    #num_grads: 1024
#    num_grads: 1
#    damp: 1e-7
#    fisher_block_size: 16
#    grad_sampler_kwargs:
#      batch_size: 8
