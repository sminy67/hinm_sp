modifiers:
  - !EpochRangeModifier
    start_epoch: 0
    end_epoch: 50

training_modifiers:
  - !LearningRateFunctionModifier
    start_epoch: 0.0
    end_epoch: 2.0
    lr_func: linear
    init_lr: 8e-5
    final_lr: 8e-6
  - !LearningRateFunctionModifier
    start_epoch: 2.0
    end_epoch: 50.0
    lr_func: cyclic_linear
    cycle_epochs: 4.0
    init_lr: 5e-5
    final_lr: 5e-6

  #- !OBSnmpairPruningModifier
  - !OBS28v64pairPermPruningModifier
    params: [
     "re:bert.encoder.layer.*.attention.self.query.weight",
     "re:bert.encoder.layer.*.attention.self.key.weight",
     "re:bert.encoder.layer.*.attention.self.value.weight",
     "re:bert.encoder.layer.*.attention.output.dense.weight",
     "re:bert.encoder.layer.*.intermediate.dense.weight",
     "re:bert.encoder.layer.*.output.dense.weight",
    ]
    init_sparsity: 0.25
    final_sparsity: 0.75
    start_epoch: 2
    end_epoch: 22
    update_frequency: 5.0
    inter_func: linear
    global_sparsity: True
    #mask_type: "5:8"
    mask_type: hinm
    num_grads: 1024
    damp: 1e-7
    fisher_block_size: 32
    grad_sampler_kwargs:
     batch_size: 16

  #- !OBSnmpairPruningModifier
  #  params: [
  #   "re:bert.encoder.layer.*.attention.self.query.weight",
  #   "re:bert.encoder.layer.*.attention.self.key.weight",
  #   "re:bert.encoder.layer.*.attention.self.value.weight",
  #   "re:bert.encoder.layer.*.attention.output.dense.weight",
  #   "re:bert.encoder.layer.*.intermediate.dense.weight",
  #   "re:bert.encoder.layer.*.output.dense.weight",
  #  ]
  #  init_sparsity: 0.375
  #  final_sparsity: 0.375
  #  start_epoch: 2
  #  end_epoch: 2
  #  update_frequency: 1.0
  #  inter_func: nm_decay
  #  global_sparsity: True
  #  #mask_type: "5:8"
  #  mask_type: unstructured
  #  num_grads: 1
  #  damp: 1e-7
  #  fisher_block_size: 32
  #  grad_sampler_kwargs:
  #   batch_size: 8
  #
  #- !OBSnmpairPruningModifier
  #  params: [
  #   "re:bert.encoder.layer.*.attention.self.query.weight",
  #   "re:bert.encoder.layer.*.attention.self.key.weight",
  #   "re:bert.encoder.layer.*.attention.self.value.weight",
  #   "re:bert.encoder.layer.*.attention.output.dense.weight",
  #   "re:bert.encoder.layer.*.intermediate.dense.weight",
  #   "re:bert.encoder.layer.*.output.dense.weight",
  #  ]
  #  init_sparsity: 0.5
  #  final_sparsity: 0.5
  #  start_epoch: 6
  #  end_epoch: 6
  #  update_frequency: 1.0
  #  inter_func: nm_decay
  #  global_sparsity: True
  #  #mask_type: "4:8"
  #  mask_type: unstructured
  #  num_grads: 1
  #  damp: 1e-7
  #  fisher_block_size: 32
  #  grad_sampler_kwargs:
  #   batch_size: 8
  #
  #- !OBSnmpairPruningModifier
  #  params: [
  #   "re:bert.encoder.layer.*.attention.self.query.weight",
  #   "re:bert.encoder.layer.*.attention.self.key.weight",
  #   "re:bert.encoder.layer.*.attention.self.value.weight",
  #   "re:bert.encoder.layer.*.attention.output.dense.weight",
  #   "re:bert.encoder.layer.*.intermediate.dense.weight",
  #   "re:bert.encoder.layer.*.output.dense.weight",
  #  ]
  #  init_sparsity: 0.625
  #  final_sparsity: 0.625
  #  start_epoch: 10
  #  end_epoch: 10
  #  update_frequency: 1.0
  #  inter_func: nm_decay
  #  global_sparsity: True
  #  #mask_type: "3:8"
  #  mask_type: unstructured
  #  num_grads: 1
  #  damp: 1e-7
  #  fisher_block_size: 32
  #  grad_sampler_kwargs:
  #   batch_size: 8
  #
  #- !OBSnmpairPruningModifier
  #  params: [
  #   "re:bert.encoder.layer.*.attention.self.query.weight",
  #   "re:bert.encoder.layer.*.attention.self.key.weight",
  #   "re:bert.encoder.layer.*.attention.self.value.weight",
  #   "re:bert.encoder.layer.*.attention.output.dense.weight",
  #   "re:bert.encoder.layer.*.intermediate.dense.weight",
  #   "re:bert.encoder.layer.*.output.dense.weight",
  #  ]
  #  init_sparsity: 0.75
  #  final_sparsity: 0.75
  #  start_epoch: 14
  #  end_epoch: 14
  #  update_frequency: 1.0
  #  inter_func: nm_decay
  #  global_sparsity: True
  #  #mask_type: "2:8"
  #  mask_type: unstructured
  #  num_grads: 1
  #  damp: 1e-7
  #  fisher_block_size: 32
  #  grad_sampler_kwargs:
  #   batch_size: 8

distillation_modifiers:
  - !DistillationModifier
     hardness: 1.0
     temperature: 2.0
     distill_output_keys: [start_logits, end_logits]